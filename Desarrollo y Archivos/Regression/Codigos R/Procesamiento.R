#Procesamiento de Datos
summary(trainReg) #Revisando los datos, vemos que todos los valores de V4 son 0
#Entonces vale la pega eliminarlos
trainReg = trainReg[,-4]
head(trainReg)

pTotal = ncol(trainReg)

variablesInutiles = c()
#Primero quiero los indices de todas las variables que tienen una correlacion < 0.3
for(i in 2:pTotal)
{
  varAct = trainReg[,i]
  correlacionAct = cor(trainReg$V1, varAct)
  if(abs(correlacionAct)< 0.05)
  {
    variablesInutiles = c(variablesInutiles, i)
  }
}

#Datos sin predictores con correlacion baja
datosSin = trainReg[,-variablesInutiles]

library(corrplot)

matrizCorrelaciones = cor(datosSin)

corrplot(matrizCorrelaciones, method = "color")


#Dado que las covarianzas son informacion redundante, y vienen en parejas usamos
#el siguiente algoritmo de Khun:
# 1. Calculate the correlation matrix of the predictors.
# 2. Determine the two predictors associated with the largest absolute pairwise
# correlation (call them predictors A and B).
# 3. Determine the average correlation between A and the other variables.
# Do the same for predictor B.
# 4. If A has a larger average correlation, remove it; otherwise, remove predic-
#   tor B.
# 5. Repeat Steps 2â€“4 until no absolute correlations are above the threshold.

#Para eso usamos una funcion
remove_high_correlation <- function(data, threshold = 0.75) {
  while (TRUE) {
    # Calculate the correlation matrix of the predictors
    corr_matrix <- cor(data)
    
    # Set the diagonal elements to zero to exclude self-correlations
    diag(corr_matrix) <- 0
    
    # Find the maximum absolute correlation
    max_corr <- max(abs(corr_matrix))
    
    # Exit the loop if no absolute correlations are above the threshold
    if (max_corr < threshold) {
      break
    }
    
    # Find the indices of the maximum absolute correlation
    indices <- which(abs(corr_matrix) == max_corr, arr.ind = TRUE)
    A <- rownames(corr_matrix)[indices[1, 1]]
    B <- colnames(corr_matrix)[indices[1, 2]]
    
    # Check if the correlation is exactly 1
    if (abs(max_corr - 1) < 1e-10) {
      warning("Variables with correlation equal to 1 found: ", A, " and ", B)
      break  # Exit the loop to avoid an infinite loop
    }
    
    # Determine the average correlation between A and the other variables
    avg_corr_A <- mean(abs(corr_matrix[A, ]))
    
    # Do the same for predictor B
    avg_corr_B <- mean(abs(corr_matrix[B, ]))
    
    # If A has a larger average correlation, remove it; otherwise, remove predictor B
    if (avg_corr_A > avg_corr_B) {
      data <- data[, !(colnames(data) %in% A)]
    } else {
      data <- data[, !(colnames(data) %in% B)]
    }
  }
  
  return(data)
}


#Datos en X pre limpieza
preX = datosSin[,-1]


datosNuevos = remove_high_correlation(preX, 0.55)
dim(datosNuevos)
head(datosNuevos)

matrizNueva = cor(datosNuevos)
corrplot(matrizNueva, method = "color")

datosY = trainReg$V1
matrizNueva
combined_df <- cbind(datosY, datosNuevos)
nombres = colnames(combined_df)
nombres[1]="V1"
colnames(combined_df) = nombres
write.csv(combined_df, "trainFS.csv", row.names = FALSE)

nombresSinV1 = nombres[-1]
testFS = testReg[,nombresSinV1]
write.csv(testFS, "testFS.csv", row.names = FALSE)






#Vamos a revisar outliers
library(mvoutlier)
 chisq.plot(combined_df[,-1])
#indicesOutliers = c(20004,27910,46287,56236,27889,2110,34351,1149,63534,56917,27912,70100,6302,684,58650,27906,20833,27917,35504,24212,75247,2600,76361,66573,70098,65917,7742,44927,47553,62980,75249,36583,35867,68112,27892,253,46894,61005,42140,51351,36562,51356,24521,65953,44282,42132,17738,70738,1749,75660,18682,3005,16918,63321,58637,73061,33480,18710,51664,66356,42405,38623,74106,70103,34536,46010,35392,26412,24317,34772,14882,6309,24484,51798,14627,69898,65962,73172,21251,6157,39791,36565,42157,65956,64524,40261,19934,63322,19880,9078,42147,76339,37982,70101,76366,25559,14580,55418,42144,53569,34527,63622,9581,36572,36582,65944,36678,3475,54001,65943,27891,72784,51357,37668,52880,43019,24472,76252,36564,76341,53885,27880,56335,62673,15814,21033,54010,54004,50018,7152,9238,52196,13732,27890,27473,36563,37890,2302,73044,28783,49555,38917,24831,59776,51269,31397,60434,58816,48814,35267,5535,27015,45624,54539,48531,13737,34827,63660,15046,19933,19744,67070,11873,62730,41961,24488,52949,7965,31760,70097,2604,35858,19341,44811,26283,39484,64993,42127,54704,32961,36764,20787,5539,38686,9763,33746,15572,37682,36717,38417,38721,51950,7023,51669,7010,29697,48866,6337,2305,31801,3422,42139,13898,29367,46144,34831,36581,49473,9087,62439,18140,50011,52185,45619,75243,50470,66540,54257,49195,52194,65981,36138,5349,73727,18195,63809,40822,55483,27881,62442,54787,4402,55434,24510,67758,34826,54254,57807,34315,36578,50155,31795,68421,2971,60112,73723,60164,38227,4242,75285,24477,55424,32973,59816,53869,66572,46576,18919,52752,65644,65951,3016,28247,74113,17542,75259,20088,27897,27909,18689,20255,25624,46844,51125,41027,36585,55422,65047,55417,61466,27207,6395,54691,51410,361,70102,57601,9095,24578,18562,67006,75267,70099,49576,60400,36846,37396,45626,5571,26415,76353,75246,20160,26414,27193,7145,1527,49220,43648,56748,27614,18149,11852,50998,42520,41006,12173,7964,65942,35017,42155,72315,11004,34825,27911,36714,62678,11415,20005,6160,40824,19884,61205,75155,62608,43642,67769,58933,9023,19292,73726,76174,36589,76166,42161,42131,12682,70618,37966,60433,26288,44187,38466,51194,44837,43534,62123,71242,58836,360,33193,71044,15817,63318,73730,14381,3015,50327,16709,44810,1395,45520,27902,135,53992,74111,20506,37210,51877,27498,13735,70623,51464,35561,57808,33990,55472,47437,66566,33716,39068,15994,11849,61661,14377,5464,16691,16715,26286,3370,19932,77466,58758,51363,1483,10863,35868,44842,22558,53570,20089,44836,51352,22785,49900,9022,50081,46496,6696,14683,37166,71104,67797,17209,5347,45487,67482,42130,49578,16723,11881,68563,16922,1643,49906,19638,31496,60549,6335,32970,12888,34320,38258,8991,46580,44821,37934,40882,65889,77765,58649,34323,72674,55809,67720,44695,51593,63310,23147,16853,6552,10113,15693,41962,34305,49570,52134,4477,56759,47369,43205,32156,53985,33409,4407,77464,77465,70615,76295,54013,2181,15477,38488,65200,63297,54788,9012,50588,54711,66349,26427,65588,1985,44812,16851,42057,8282,72309,42642,55595,43988,42647,12178,44929,27975,45189,28448,38408,42133,29679,46856,36847,27877,20467,53825,42657,2743,75812,50061,9224,61017,7461,43202,7141,10550,27511,62130,74115,43147,16931,39774,447,3021,37675,7949,51765,18443,34836,44909,75148,63401,72986,6543,6646,63325,42149,30834,58647,31500,31714,57599,22608,43199,74938,9643,75149,20267,27262,58631,38487,9971,51901,70925,75628,37099,39067,36030,7746,26290,70453,37178,37366,19688,27507,34829,72998,36600,19553,54008,42137,75810,51195,36597,20082,62039,53715,24481,37801,4871,62666,24829,75260,25318,36559,43125,31638,65959,33484,77009,30460,53887,68166,12811,20621,26426,7148,54545,11420,9962,15051,34332,74112,63324,67756,53576,18706,20826,35854,52882,19093,56761,63413,30828,359,14378,27646,14246,40990,42515,75144,55419,23100,62131,29056,61512,27914,64390,43635,13285,4333,68851,9420,41054,53827,3376,49846,54537,40421,9764,72780,22411,62041,6073,36845,4308,48498,51761,5529,33090,16734,48191,54878,45199,62444,10541,5024,36579,45632,1265,40973,49806,7126,68938,8719,8365,62040,26408,43639,34858,22123,42129,65321,46826,6156,16930,29361,23887,12796,60446,62676,43201,67930,6821,56692,17995,10199,37375,19895,56685,14863,4311,38724,61787,63856,74528,26432,56543,33091,41046,12288,17128,32097,77467,51879,58941,18683,49993,6551,12662,65650,31772,41032,77002,71892,51270,41043,12076,63623,31898,16173,12067,5416,41018,59062,36566,25043,69306,26289,3022,27901,45924,11889,19056,14379,64823,40755,45179,75804,60014,18346,4309,77599,12929,1624,61817,67757,12699,10047,42516,10068,55460,26403,2993,74114,57004,49579,58932,65050,12650,64830,43198,43211,48807,23579,43197,71603,6785,75726,65948,40989,1308,15717,43195,13554,67771,57681,43409,77777,24834,56999,67933,47865,54974,36037,16921,65954,4232,6818,47344,46646,73584,36561,53890,36411,27904,41513,8614,27512,47442,6161,5577,58937,3582,37535,72964,58936,52059,31178,57815,52886,49789,19164,65977,33473,63769,72225,74188,53588,12832,12670,76472,19273,367,75575,45802,20007,58985,42151,36584,6556,21023,44846,37686,30833,35018,60114,41030,59777,16707,43229,62127,3165,4233,24698,48198,22557,43165,30463,36131,12877,10677,67353,33413,19033,48777,31781,36602,70907,63859,76243)
indicesOutliers = indices$Indice

#Datos train con FS y sin Outliers
trainFSOut = combined_df[-indicesOutliers,]

# Assuming 'combined_df' is your data frame
write.csv(trainFSOut, "trainFSOut.csv", row.names = FALSE)

#Datos de train son solo Outliers (para metodos que no tengan problemas de dim)
trainOut = trainReg[-indicesOutliers,]
write.csv(trainOut, "trainOut.csv", row.names = FALSE)

#Ahora me interesa sacar el test con solo lo las variables seleccionadas (para NN)
nombresSinV1 = nombres[-1]
testFS = testReg[,nombresSinV1]
write.csv(testFS, "testFS.csv", row.names = FALSE)
